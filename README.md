# Understanding and Harnessing Sparsity in Unified Multimodal Models

---

[![Task](https://img.shields.io/badge/Task-Unified%20Multimodal-blue)](#)
[![Focus](https://img.shields.io/badge/Focus-Sparse%20Activation-green)](#)
[![Python](https://img.shields.io/badge/Python-3.10+-brightgreen)](#)

This repository contains the code and experiments for **Efficient Unified Multimodal Modeling (Efficient-UG)**, a study on redundancy and dynamic sparsity in unified models that jointly support multimodal **understanding** and **generation**.

<p align="center">
  <img src="efficient_ug.svg" alt="Efficient-UG overview" width="72%">
</p>

## âš¡ TL;DR
- Unified multimodal models show strong task-dependent redundancy across understanding and generation paths.
- Generation modules are more compression-sensitive than understanding modules.
- Sparse expert activation (MoE-style adaptation) recovers generation quality with lower active parameters.
- The resulting BAGEL variant keeps competitive performance while activating roughly half of parameters.

## ğŸ” Overview
Unified multimodal models promise one architecture for reasoning and content generation, but this unification introduces non-uniform compute demand across tasks and samples.

Efficient-UG analyzes these inefficiencies through training-free probing and sparse adaptation, covering:
1. **Depth Pruning** (layer dropping),
2. **Width Reduction** (neuron partitioning),
3. **Expert Partitioning** for sparse MoE preparation.

## ğŸ“° News
- Feb 2026: README reorganized with a cleaner research-repo layout and command flow.

## âœ¨ Why This Repo
This codebase unifies and adapts model components from:
- [BAGEL](https://github.com/ByteDance-Seed/Bagel)
- [Ming-Omni](https://github.com/inclusionAI/Ming/tree/main)
- [Qwen-Image](https://github.com/QwenLM/Qwen-Image)

Key adapted entry files:
- `modeling/bagel/bagel.py`
- `Ming/modeling_bailingmm.py`
- `diffusers/pipelines/qwenimage/modeling_qwen2_5_vl.py`

These adaptations provide consistent layer/dimension interfaces for systematic pruning and sparse computation studies.

## ğŸ“¦ Installation
```bash
conda create -n efficient_ug python=3.10 -y
conda activate efficient_ug

pip install -r requirements.txt
```

## ğŸš€ Quick Start
### 1) Depth Pruning Evaluation
Understanding:
```bash
bash eval/vlm/evaluate_ld.sh
```

Generation:
```bash
bash scripts/eval/bagel/run_geneval_ld.sh
bash scripts/eval/ming/run_geneval_ld.sh
bash scripts/eval/qwen/run_geneval_ld.sh
```

### 2) Width Reduction Evaluation
Understanding:
```bash
bash eval/vlm/evaluate_wr.sh
```

Generation:
```bash
bash scripts/eval/bagel/run_geneval_wr.sh
bash scripts/eval/ming/run_geneval_wr.sh
bash scripts/eval/qwen/run_geneval_wr.sh
```

### 3) Neuron Partitioning Example
```bash
python neuron_partition.py
```

### 4) Dense-to-Sparse Expert Conversion
Use:
- `dense2sparse.ipynb`

This notebook demonstrates converting dense generation modules into sparse expert-style structures for adaptive activation.

## ğŸ§  Core Methods
1. **Depth Pruning via Layer Dropping**
- Reduces inference depth while preserving multimodal understanding quality as much as possible.

2. **Width Reduction via Neuron Partitioning**
- Identifies and prunes less active neurons for task-specific compactness.

3. **Expert Partitioning for MoE Preparation**
- Splits generation modules into experts for sparse activation and later expert-based adaptation.

## ğŸ—‚ï¸ Repository Layout
```text
SparseUnifiedModel/
â”œâ”€â”€ modeling/                      # Core model definitions (BAGEL, Ming-Omni, Qwen-Image)
â”‚   â””â”€â”€ bagel/
â”œâ”€â”€ Ming/                          # Ming-Omni related modeling files
â”‚   â””â”€â”€ modeling_bailingmm.py
â”œâ”€â”€ diffusers/                     # Adapted Qwen-Image modules and pipelines
â”‚   â””â”€â”€ pipelines/qwenimage/
â”œâ”€â”€ data/                          # Data preprocessing utilities
â”œâ”€â”€ eval/                          # Evaluation for understanding and generation
â”‚   â””â”€â”€ vlm/
â”œâ”€â”€ scripts/                       # Shell launchers for different models/tasks
â”‚   â””â”€â”€ eval/
â”œâ”€â”€ utils/                         # Shared utility functions
â”œâ”€â”€ dense2sparse.ipynb             # Dense-to-sparse MoE preparation demo
â”œâ”€â”€ neuron_partition.py            # Neuron importance analysis and partitioning
â”œâ”€â”€ inference.ipynb                # Inference/pruning walkthrough
â”œâ”€â”€ inferencer.py                  # Unified inference interface
â”œâ”€â”€ efficient_ug.svg               # Project figure
â”œâ”€â”€ prompts.txt                    # Example prompts
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“„ Citation
If you use this repository in your research, please cite the associated paper when available.

## ğŸ“¬ Contact
- `shwai.he@bytedance.com`
- `sheny@bytedance.com`
