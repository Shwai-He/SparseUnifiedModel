{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qwen-Image & Qwen-Image Edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers\n",
    "\n",
    "# if not hasattr(diffusers.utils.constants, \"MIN_PEFT_VERSION\"):\n",
    "# diffusers.utils.constants.MIN_PEFT_VERSION = \"0.7.0\"  # PEFTæœ€ä½ç‰ˆæœ¬è¦æ±‚\n",
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen-Image\"\n",
    "model_name = \"your_model_path\"  \n",
    "# Load the pipeline\n",
    "if torch.cuda.is_available():\n",
    "    torch_dtype = torch.bfloat16\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    torch_dtype = torch.float32\n",
    "    device = \"cpu\"\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(model_name, torch_dtype=torch_dtype)\n",
    "pipe = pipe.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipe.transformer\n",
    "total = 0\n",
    "for name, p in model.named_parameters():\n",
    "    total += p.numel() \n",
    "\n",
    "total / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "sparse_ratio = 0.3\n",
    "\n",
    "def count_parameters(model, only_trainable=False, exclude_label=None):\n",
    "    \"\"\"\n",
    "    ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡\n",
    "    Args:\n",
    "        model: nn.Module, æ¨¡å‹\n",
    "        only_trainable: bool, æ˜¯å¦åªç»Ÿè®¡å¯è®­ç»ƒå‚æ•°\n",
    "        exclude_label: list[str], è¦æ’é™¤çš„å‚æ•°åå…³é”®è¯\n",
    "    Returns:\n",
    "        å‚æ•°æ€»æ•° (int)\n",
    "    \"\"\"\n",
    "    if exclude_label is None:\n",
    "        exclude_label = []\n",
    "\n",
    "    total = 0\n",
    "    for name, p in model.named_parameters():\n",
    "        if any(label in name for label in exclude_label):\n",
    "            continue\n",
    "        if only_trainable and not p.requires_grad:\n",
    "            continue\n",
    "        print(name)\n",
    "        total += p.numel() * sparse_ratio if \"mlp\" in name else p.numel()\n",
    "    return total\n",
    "\n",
    "\n",
    "exclude_label = [\"moe_gen\", \"vit_model\", \"connector\", \"vae2llm\", \"llm2vae\", \"time_embedder\", \"pos_embed\"]\n",
    "exclude_label = [\"visual\"]\n",
    "\n",
    "model = pipe.text_encoder\n",
    "print(\"Total Parameters:\", count_parameters(model, exclude_label=exclude_label) /  1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.pipelines.qwenimage.modeling_qwen2_5_vl import Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "model_path = f\"{model_name}/text_encoder\"\n",
    "pipe.text_encoder = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path, torch_dtype=\"auto\", device_map=\"auto\", trust_remote_code=True, \n",
    ").to(torch_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positive_magic = {\n",
    "    \"en\": \", Ultra HD, 4K, cinematic composition.\", # for english prompt\n",
    "    \"zh\": \", è¶…æ¸…ï¼Œ4Kï¼Œç”µå½±çº§æ„å›¾.\" # for chinese prompt\n",
    "}\n",
    "\n",
    "# Generate image\n",
    "prompt = '''A coffee shop entrance features a chalkboard sign reading \"Qwen Coffee ğŸ˜Š $2 per cup,\" with a neon light beside it displaying \"é€šä¹‰åƒé—®\". Next to it hangs a poster showing a beautiful Chinese woman, and beneath the poster is written \"Ï€â‰ˆ3.1415926-53589793-23846264-33832795-02384197\". Ultra HD, 4K, cinematic composition'''\n",
    "negative_prompt = \" \" # using an empty string if you do not have specific concept to remove\n",
    "\n",
    "# Generate with different aspect ratios\n",
    "aspect_ratios = {\n",
    "    \"1:1\": (1328, 1328),\n",
    "    \"16:9\": (1664, 928),\n",
    "    \"9:16\": (928, 1664),\n",
    "    \"4:3\": (1472, 1140),\n",
    "    \"3:4\": (1140, 1472),\n",
    "    \"3:2\": (1584, 1056),\n",
    "    \"2:3\": (1056, 1584),\n",
    "}\n",
    "\n",
    "width, height = aspect_ratios[\"16:9\"]\n",
    "num_inference_steps = 2\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt=prompt + positive_magic[\"en\"],\n",
    "    negative_prompt=negative_prompt,\n",
    "    width=width,\n",
    "    height=height,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    true_cfg_scale=4.0,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(42)\n",
    ")\n",
    "\n",
    "image = outputs.images[0]\n",
    "display(image)"
   ]
  }
 ],
 "metadata": {
  "fileId": "0af0d481-aa2c-4074-92e8-eac33aa67965",
  "filePath": "/mnt/bn/seed-aws-va/shwai.he/cdt-hf/demo_qwen.ipynb",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
